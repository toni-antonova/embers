# Lumen ‚Äî Tracking Work History (Part 2)

A continuation of the linear log of all meaningful code, architecture, and configuration changes as we develop the Lumen pipeline and infrastructure. (Continued from `tracking_work_history.md` entry #44).

## Table of Contents

| # | Entry | Date |
|---|-------|------|
| 45 | Deployment Bug Fix Chain ‚Äî Docker, Dependencies, Python 3.13 | 2026-02-22 |
| 46 | Cloud Run Startup Probe + Background Model Loading | 2026-02-22 |
| 47 | MILESTONE: First Successful Cloud Run Deployment üöÄ | 2026-02-22 |
| 48 | MILESTONE: SDXL Turbo API ‚Äî Live Text-to-Image Inference üèéÔ∏è | 2026-02-22 |
| 49 | Prompt 03 Audit ‚Äî AutoPipelineForText2Image Revert + Quality Verification | 2026-02-22 |
| 50 | PartCrafter Integration ‚Äî Image ‚Üí Part-Decomposed Meshes ‚Üí Point Clouds | 2026-02-22 |
| 51 | Deploy Fix: torch-cluster + Client HTTP Integration | 2026-02-22 |
| 52 | MILESTONE: PartCrafter ‚Äî Live 3D Mesh Generation Pipeline üß© | 2026-02-22 |
| 53 | S08: Cache Layer Hardening + Pre-Generation | 2026-02-22 |
| 54 | A2: Parametric Primitive Library + Shader Enhancements | 2026-02-22 |

---

<details>
<summary><strong>45. Deployment Bug Fix Chain ‚Äî Docker, Dependencies, Python 3.13</strong></summary>

**Date:** 2026-02-22

<details>
<summary><strong>Issue</strong></summary>

First Cloud Run build revealed a chain of 9 blocking issues that had to be fixed iteratively ‚Äî each fix uncovered the next failure. These are the kinds of incompatibilities that only surface in a real CUDA container build, not in local dev:

| # | Commit | Error | Root Cause |
|---|--------|-------|------------|
| 1 | `2dd3680` | `E: Unable to locate package libegl1-mesa` | Ubuntu 24.04 (Noble) restructured Mesa packages |
| 2 | `ed65d59` | `WORKDIR: permission denied` | Directory created as root, then `USER appuser` couldn't write |
| 3 | `1831fe8` | `AttributeError: total_mem` | PyTorch 2.10 renamed `CudaDeviceProperties.total_mem` ‚Üí `total_memory` |
| 4 | `d8541b5` | `ModuleNotFoundError: diffusers` | `diffusers` + `accelerate` were dev-only deps, missing from production |
| 5 | `514d555` | ~5 min cold builds | No Docker layer caching ‚Äî full `uv sync` on every build |
| 6 | `6354f71` | `ImportError: transformers` | `transformers` + `safetensors` were transitive deps, not declared |
| 7 | `ff8b4e7` | `ImportError: sentencepiece` | `diffusers` lazily imports HunyuanDiT ‚Üí MT5Tokenizer ‚Üí sentencepiece |
| 8 | `a25c19c` | `sentencepiece` build fails on Python 3.14 | No pre-built wheels for Python 3.14; `AutoPipelineForText2Image` eagerly imports all pipelines |
| 9 | `0918bba` | Stale lockfile | `uv.lock` needed regeneration after Python version change |

</details>

<details>
<summary><strong>Fix</strong></summary>

**Docker fixes:**
- `libegl1-mesa` ‚Üí `libegl1`, `libgl1-mesa-glx` ‚Üí `libgl1` (Ubuntu 24.04 package rename)
- Added `RUN mkdir -p /home/appuser/app && chown appuser:appuser /home/appuser/app` before `USER appuser`

**API fix:**
- `torch.cuda.get_device_properties(0).total_mem` ‚Üí `.total_memory` (PyTorch 2.10 rename)

**Dependency fixes:**
- Added explicit production deps: `diffusers>=0.33`, `accelerate>=1.7`, `transformers`, `safetensors`, `sentencepiece`, `protobuf`
- These were all transitive via `torch` or `diffusers` but not declared ‚Äî worked locally because dev environment had them

**Python version downgrade:**
- Python 3.14 ‚Üí 3.13 in `Dockerfile` and `pyproject.toml`
- `sentencepiece` has no pre-built wheel for 3.14, and building from source fails in the CUDA container
- Python 3.13 has full wheel support for the entire ML stack

**Import fix:**
- `AutoPipelineForText2Image` ‚Üí `StableDiffusionXLPipeline` ‚Äî avoids eager import of all pipeline modules (HunyuanDiT, Kandinsky, etc.)

**Build performance:**
- Added `--cache-from` layer caching + BuildKit `--mount=type=cache` for uv wheel persistence
- Code-only changes now skip the 2GB dependency layer ‚Üí ~30-60s builds vs ~5 min

</details>

<details>
<summary><strong>Files Changed</strong></summary>

| File | Changes |
|------|---------|
| `server/Dockerfile` | Mesa packages, WORKDIR permission, Python 3.14‚Üí3.13, BuildKit syntax |
| `server/pyproject.toml` | Added 6 explicit deps, Python version 3.14‚Üí3.13 |
| `server/uv.lock` | Regenerated for Python 3.13 (333 lines added) |
| `server/app/models/sdxl_turbo.py` | `AutoPipelineForText2Image` ‚Üí `StableDiffusionXLPipeline` |
| `server/app/models/registry.py` | `total_mem` ‚Üí `total_memory` |
| `infrastructure/cloudbuild.yaml` | Layer caching, BuildKit, separate push steps |

</details>

<details>
<summary><strong>Outcome</strong></summary>

- Docker image builds successfully on Cloud Build ‚úÖ
- All ML dependencies resolve and import cleanly ‚úÖ
- Python 3.13 provides full wheel coverage ‚úÖ
- Rebuild time reduced from ~5 min to ~30-60s for code-only changes ‚úÖ
- 9 commits: `2dd3680` through `0918bba` ‚úÖ

</details>

</details>

---

<details>
<summary><strong>46. Cloud Run Startup Probe + Background Model Loading</strong></summary>

**Date:** 2026-02-22

<details>
<summary><strong>Issue</strong></summary>

Cloud Run was killing the container during SDXL Turbo model download. The startup probe allowed 24 √ó 5s = 120s, but downloading and loading a ~3GB model from Hugging Face takes 3-5 minutes on Cloud Run's network. Meanwhile, the model loading was synchronous in `lifespan()`, which meant uvicorn couldn't bind the port until loading finished ‚Äî so the startup probe had nothing to probe.

</details>

<details>
<summary><strong>Fix</strong></summary>

**Terraform** (`cloud_run.tf`):
```diff
  startup_probe {
-   failure_threshold     = 24
-   period_seconds        = 5
+   failure_threshold     = 60
+   period_seconds        = 10
+   initial_delay_seconds = 0
  }
```
Total timeout: 60 √ó 10s = **600s** (10 minutes). `initial_delay_seconds=0` so probes begin immediately.

**Application** (`main.py`):
- Moved model loading from synchronous `lifespan()` to an `asyncio` background task via `loop.run_in_executor()`
- uvicorn binds port immediately ‚Üí Cloud Run startup probe succeeds on first `/health` check
- `/health/ready` returns **503** until model registry is populated
- `/generate` returns `ModelNotLoadedError` until registry has `sdxl_turbo`
- Model loads in a background thread without blocking the event loop

</details>

<details>
<summary><strong>Files Changed</strong></summary>

| File | Changes |
|------|---------|
| `infrastructure/terraform/cloud_run.tf` | Startup probe: 120s ‚Üí 600s timeout |
| `server/app/main.py` | Background model loading via `asyncio` + `run_in_executor` |

</details>

<details>
<summary><strong>Outcome</strong></summary>

- Container starts and passes health checks within seconds ‚úÖ
- Model loads in background without blocking request handling ‚úÖ
- `/health/ready` correctly reports 503 until models are loaded ‚úÖ
- 600s startup window accommodates even slow model downloads ‚úÖ
- Committed as `33656fe` ‚úÖ

</details>

</details>

---

<details>
<summary><strong>47. MILESTONE: First Successful Cloud Run Deployment üöÄ</strong></summary>

**Date:** 2026-02-22

<details>
<summary><strong>What Was Accomplished</strong></summary>

The Lumen ML pipeline server is now **live on Google Cloud Run** with GPU acceleration. This is the first successful end-to-end deployment ‚Äî from Terraform infrastructure provisioning through Docker image build to a running, authenticated API endpoint.

Since the infrastructure and server skeleton were built (entries #39‚Äì#44), **9 deployment bug fixes** (entry #45) and **1 startup architecture fix** (entry #46) were needed to get from "code that works locally" to "container running in production."

</details>

<details>
<summary><strong>What's Deployed</strong></summary>

| Component | Detail |
|-----------|--------|
| **Compute** | Cloud Run v2, `us-east4`, NVIDIA L4 GPU (24GB VRAM) |
| **Container** | CUDA 12.8 + Python 3.13 + FastAPI + uvicorn |
| **Model** | SDXL Turbo (`stabilityai/sdxl-turbo`) loaded on startup |
| **Auth** | API key via Secret Manager + `secrets.compare_digest()` |
| **Rate Limiting** | 60 req/min per IP via slowapi |
| **CORS** | Configurable per environment via `ALLOWED_ORIGINS` |
| **Health** | `/health` (liveness) + `/health/ready` (readiness, 503 until model loaded) |
| **Endpoints** | `POST /generate`, `POST /debug/generate-image` |
| **Caching** | In-memory LRU (100 items) + Cloud Storage backend |
| **Logging** | structlog ‚Üí Cloud Logging (JSON renderer) |
| **Build** | Cloud Build with BuildKit layer caching (~30-60s rebuilds) |

</details>

<details>
<summary><strong>Deployment Journey</strong></summary>

The path from `terraform apply` to a healthy container required solving problems across 4 layers:

1. **OS layer** ‚Äî Ubuntu 24.04 renamed Mesa packages, Docker WORKDIR permissions
2. **Python layer** ‚Äî Python 3.14 lacks wheel support for ML stack ‚Üí downgraded to 3.13
3. **Dependency layer** ‚Äî 6 transitive deps needed explicit declaration; diffusers eager-imports all pipeline modules
4. **Application layer** ‚Äî PyTorch API rename (`total_mem`), synchronous model loading blocked startup probes

Each layer had to be fixed in order ‚Äî you can't discover the Python 3.14 issue until the OS packages install, you can't discover the import issue until the deps install, etc.

</details>

<details>
<summary><strong>What's Next</strong></summary>

- **Prompt 04: PartCrafter integration** ‚Äî wire `reference_image` from SDXL Turbo into the part segmentation model
- **Frontend ‚Üî API connection** ‚Äî connect the dots client to the deployed `/generate` endpoint
- **Cost monitoring** ‚Äî set up billing alerts for GPU usage
- **Model caching** ‚Äî pre-bake model weights into the Docker image to eliminate cold-start download

</details>

</details>

---

<details>
<summary><strong>48. MILESTONE: SDXL Turbo API ‚Äî Live Text-to-Image Inference on Cloud Run üèéÔ∏è</strong></summary>

**Date:** 2026-02-22

<details>
<summary><strong>What Was Accomplished</strong></summary>

The Lumen ML pipeline's text-to-image inference endpoint is **fully operational in production**. The `POST /debug/generate-image` endpoint accepts authenticated requests, runs them through the `stabilityai/sdxl-turbo` model on an NVIDIA L4 GPU, and returns generated PNG images.

This milestone validates the entire stack end-to-end: Terraform-provisioned infrastructure ‚Üí Cloud Build Docker image ‚Üí Cloud Run container with GPU ‚Üí FastAPI server ‚Üí SDXL Turbo model inference ‚Üí authenticated image response.

We verified by submitting **10 prompts of varying sentence complexity** (all <10 words) to the live production endpoint.

</details>

<details>
<summary><strong>Visual Verification ‚Äî 10 Live API Generations</strong></summary>

| Prompt | Result |
|--------|--------|
| `a corrupt politician eats noodles` | ![Image 1](milestones/sdxl-turbo-api/image_01.png) |
| `a fast red sports car` | ![Image 2](milestones/sdxl-turbo-api/image_02.png) |
| `sunset over the calm ocean` | ![Image 3](milestones/sdxl-turbo-api/image_03.png) |
| `astronaut floating in deep space` | ![Image 4](milestones/sdxl-turbo-api/image_04.png) |
| `cat playing with red ball` | ![Image 5](milestones/sdxl-turbo-api/image_05.png) |
| `a futuristic cyberpunk neon city` | ![Image 6](milestones/sdxl-turbo-api/image_06.png) |
| `freshly baked chocolate chip cookies` | ![Image 7](milestones/sdxl-turbo-api/image_07.png) |
| `a wise old forest owl` | ![Image 8](milestones/sdxl-turbo-api/image_08.png) |
| `neon lights reflecting in puddle` | ![Image 9](milestones/sdxl-turbo-api/image_09.png) |
| `a glowing mushroom in forest` | ![Image 10](milestones/sdxl-turbo-api/image_10.png) |

</details>

<details>
<summary><strong>Performance Notes</strong></summary>

- **Cold start:** ~3‚Äì5 min (model download + VRAM loading on first container spin-up)
- **Warm inference:** Sub-second per image (SDXL Turbo's 1-step distilled architecture)
- **Auth:** API key via `X-API-Key` header, validated with `secrets.compare_digest()`
- **Rate limiting:** 60 req/min per IP via slowapi

</details>

<details>
<summary><strong>Live Stack</strong></summary>

```
curl -X POST https://lumen-pipeline-gldbd4c4ka-uk.a.run.app/debug/generate-image \
  -H "Content-Type: application/json" \
  -H "X-API-Key: <key>" \
  -d '{"text": "your prompt here"}' \
  --output result.png
```

| Layer | Detail |
|-------|--------|
| **Compute** | Cloud Run v2, `us-east4`, NVIDIA L4 (24GB VRAM) |
| **Runtime** | CUDA 12.8 + Python 3.13 + FastAPI + uvicorn |
| **Model** | `stabilityai/sdxl-turbo` via `AutoPipelineForText2Image` |
| **Auth** | Secret Manager ‚Üí `X-API-Key` header |
| **Infra** | Terraform IaC, Cloud Build CI, Artifact Registry |

</details>

</details>

---

<details>
<summary><strong>49. Prompt 03 Audit ‚Äî AutoPipelineForText2Image Revert + Quality Verification</strong></summary>

**Date:** 2026-02-22

<details>
<summary><strong>What Was Done</strong></summary>

Systematic audit of all Prompt 03 requirements against the implementation. During the audit, identified and fixed one inconsistency: `sdxl_turbo.py` was using `StableDiffusionXLPipeline` (a workaround from the Python 3.14 era, entry #45) while the test suite mocked `AutoPipelineForText2Image`. Since the Dockerfile was already downgraded to Python 3.13 (which resolves the root cause), reverted to the canonical `AutoPipelineForText2Image` ‚Äî the Stability AI recommended API.

</details>

<details>
<summary><strong>Changes</strong></summary>

| File | Changes |
|------|---------|
| `server/app/models/sdxl_turbo.py` | `StableDiffusionXLPipeline` ‚Üí `AutoPipelineForText2Image` (import + usage, 2 lines) |

</details>

<details>
<summary><strong>Audit Results</strong></summary>

All 18 requirements from the Prompt 03 spec verified against the implementation:

| Category | Requirements | Status |
|----------|-------------|--------|
| Model wrapper (`sdxl_turbo.py`) | 16 items: import, model ID, dtype, device, progress bar, guidance, output size, timing, OOM, inference_mode, protocol props, VRAM logging, docstrings, structlog | ‚úÖ All met |
| Architecture wiring (`main.py`) | Background loading via `run_in_executor`, `skip_model_load` check, registry.register | ‚úÖ Exceeds spec |
| Pipeline integration (`pipeline.py`) | Conditional SDXL usage, canonical prompt, logging, mock fallback, TODO for Prompt 04 | ‚úÖ All met |
| Debug endpoint (`debug.py`) | `POST /debug/generate-image`, Pydantic request, PNG response, 503 when not loaded | ‚úÖ All met |
| Tests (`test_sdxl_turbo.py`) | Spec asked for 3 test areas; implementation has 14 tests across 5 test classes | ‚úÖ Exceeds spec |

</details>

<details>
<summary><strong>Verification</strong></summary>

- All **14 tests passed** locally (`uv run pytest tests/test_sdxl_turbo.py -v` ‚Äî 0.58s) ‚úÖ
- Deployed Cloud Run endpoint returned **HTTP 200** with a valid **512√ó512 RGB PNG** (308KB, ~1s) ‚úÖ

</details>

</details>

---

<details>
<summary><strong>50. PartCrafter Integration ‚Äî Image ‚Üí Part-Decomposed Meshes ‚Üí Point Clouds</strong></summary>

**Date:** 2026-02-22

<details>
<summary><strong>What Was Done</strong></summary>

Integrated PartCrafter (NeurIPS 2025) into the Lumen pipeline as the second ML model. The pipeline now generates **real 3D geometry** instead of mock spherical point clouds:

**Text ‚Üí SDXL Turbo image ‚Üí BriaRMBG background removal ‚Üí PartCrafter part-decomposed meshes ‚Üí surface-sampled point cloud with semantic part IDs**

Key design decisions:
- **BriaRMBG as separate timed step** ‚Äî SDXL Turbo images have backgrounds; PartCrafter expects white-bg. RMBG timing is logged independently.
- **Vendored PartCrafter** ‚Äî Not pip-installable, so downloaded as a tarball pinned to commit SHA `269bd41` for reproducible Docker builds.
- **Real mesh counting** ‚Äî None outputs from PartCrafter decoding failures are replaced with 1-vertex dummy meshes. Part count validation counts only *real* meshes (>1 vertex).
- **VRAM budget logging** ‚Äî After loading both SDXL Turbo (~2 GB) and PartCrafter (~4 GB), `main.py` logs allocated/total/free GPU memory.

</details>

<details>
<summary><strong>Changes</strong></summary>

| File | Changes |
|------|---------|
| `server/app/models/partcrafter.py` | **NEW** ‚Äî `PartCrafterModel` wrapper implementing `ImageToPartsModel` protocol. BriaRMBG preprocessing, `@torch.inference_mode()`, OOM handling, structlog timing. |
| `server/app/services/pipeline.py` | Primary pipeline: SDXL Turbo ‚Üí PartCrafter ‚Üí `sample_from_part_meshes()`. Real-mesh filtering, per-step `perf_counter` timing, fallback to mock. Returns 4-tuple `(positions, part_ids, part_names, pipeline)`. |
| `server/app/main.py` | PartCrafter registration in `_load_models()` + VRAM budget logging after both models load. |
| `server/Dockerfile` | Vendored PartCrafter via `curl` tarball at pinned SHA. Added `PYTHONPATH` for `from src.*` imports. |
| `server/app/routes/debug.py` | `POST /debug/generate-mesh` ‚Äî runs full pipeline, returns timing breakdown + part counts + vertex counts. |
| `server/pyproject.toml` | 10 inference-only deps: `einops`, `omegaconf`, `jaxtyping`, `typeguard`, `peft`, `colormaps`, `opencv-python`, `scikit-image`, `scikit-learn`, `huggingface-hub`. |
| `server/tests/test_partcrafter.py` | **NEW** ‚Äî 32 tests across 8 classes: protocol, generation, None handling, point sampling, pipeline fallback, debug endpoint, encoding round-trips, registry, cache. |

</details>

<details>
<summary><strong>Verification</strong></summary>

- **152 tests pass** (32 new + 120 existing) ‚Äî `uv run pytest tests/ -v` ‚Äî 2.30s ‚úÖ
- **0 regressions** in existing test suite ‚úÖ
- **Runtime GPU smoke test pending** ‚Äî needs Docker build + deploy to verify PartCrafter + SDXL Turbo coexist on L4 GPU

</details>

</details>

---

<details>
<summary><strong>51. Deploy Fix: torch-cluster + Client HTTP Integration (Prompt 06)</strong></summary>

**Date:** 2026-02-22

<details>
<summary><strong>What Was Done</strong></summary>

Two distinct pieces of work in one session:

**A) Server Deploy Fix ‚Äî PartCrafter `torch_cluster` dependency**
- PartCrafter model loading failed on Cloud Run due to missing `torch_cluster` (PyTorch Geometric extension)
- Added `torch-cluster` to `pyproject.toml` with `[tool.uv.extra-build-dependencies]` for build-time torch access
- Added `lets preflight` command ‚Äî verifies all ML imports resolve on CPU before committing to a slow Cloud Build
- Added `.gcloudignore` ‚Äî reduces Cloud Build upload from 1.8 GB to ~50 MB

**B) Client HTTP Integration (Prompt 06) ‚Äî `ServerClient` + `ServerShapeAdapter`**
- **`ServerClient.ts`** ‚Äî HTTP client with `AbortController` cancellation (latest-wins), 10s timeout, base64‚ÜíFloat32Array/Uint8Array decoding, `warmUp()` for Cloud Run cold start
- **`ServerShapeAdapter.ts`** ‚Äî Expands 2,048 server points ‚Üí 16,384 DataTexture pixels. First 2,048 are exact positions, remaining 14,336 use modular assignment with ¬±0.02 jitter
- **`ParticleSystem.setTargetTexture()`** ‚Äî Accepts raw DataTexture (bypasses name-based MorphTargets lookup)
- **`MorphTargets.hasTarget()`** ‚Äî Checks if local procedural shape exists
- **`SemanticBackend.ts`** ‚Äî Server fallback: if `hasTarget()` is false and `ServerClient` is configured, fetches from server async, falls back to closest local shape on failure
- **`Canvas.tsx`** ‚Äî Creates `ServerClient` singleton from `VITE_LUMEN_SERVER_URL`/`VITE_LUMEN_API_KEY` env vars, calls `warmUp()` on mount

</details>

<details>
<summary><strong>Changes</strong></summary>

| File | Changes |
|------|---------|
| `server/pyproject.toml` | Added `torch-cluster` + `[tool.uv.extra-build-dependencies]` |
| `server/lets.yaml` | Added `lets preflight` import verification command |
| `.gcloudignore` | **NEW** ‚Äî Excludes `.venv`, `node_modules`, `.git` from Cloud Build |
| `src/services/ServerClient.ts` | **NEW** ‚Äî HTTP client with abort, timeout, base64 decode |
| `src/engine/ServerShapeAdapter.ts` | **NEW** ‚Äî 2048‚Üí16384 expansion + part ID textures |
| `src/engine/ParticleSystem.ts` | Added `setTargetTexture(tex, label)` |
| `src/engine/MorphTargets.ts` | Added `hasTarget(name)` |
| `src/services/SemanticBackend.ts` | Server fallback path + `requestServerShape()` |
| `src/components/Canvas.tsx` | ServerClient singleton + warmUp on mount |
| `.env.example` | **NEW** ‚Äî Vite env vars for server URL and API key |
| `src/__tests__/ServerClient.test.ts` | **NEW** ‚Äî 8 tests |
| `src/__tests__/ServerShapeAdapter.test.ts` | **NEW** ‚Äî 8 tests |

</details>

<details>
<summary><strong>Verification</strong></summary>

- TypeScript compiles clean (`npx tsc --noEmit`) ‚úÖ
- **335 tests pass** (16 new + 319 existing), 0 regressions ‚úÖ
- `lets preflight` verifies all server ML imports locally on CPU ‚úÖ
- Commits: `609cba3` (torch-cluster + deploy), `94ef3bd` (client HTTP integration)

</details>

</details>

---

<details>
<summary><strong>52. MILESTONE: PartCrafter ‚Äî Live 3D Mesh Generation Pipeline üß©</strong></summary>

**Date:** 2026-02-22

<details>
<summary><strong>What Was Accomplished</strong></summary>

The Lumen server now has a **complete text-to-3D pipeline**: spoken word ‚Üí SDXL Turbo reference image ‚Üí BriaRMBG background removal ‚Üí PartCrafter part-decomposed meshes ‚Üí surface-sampled point cloud with semantic part IDs. This is the core ML pipeline that powers the entire visualization system.

PartCrafter (NeurIPS 2025) takes a single reference image and produces **multiple separate meshes**, each representing a semantic part of the object (e.g., legs, seat, back for a chair). This gives us labeled 3D geometry ‚Äî not just a point cloud, but a point cloud where every point knows which part of the object it belongs to.

**Pipeline:** `text ‚Üí SDXL Turbo (512√ó512 image, ~1s) ‚Üí BriaRMBG (background removal) ‚Üí PartCrafter (part meshes, ~30-60s) ‚Üí point_sampler (2048 labeled points)`

</details>

<details>
<summary><strong>Reference Images ‚Äî SDXL Turbo Stage (Live API)</strong></summary>

These reference images are generated by the live production API (`POST /debug/generate-image`) and feed into PartCrafter for 3D reconstruction. Each prompt uses `get_canonical_prompt()` to produce an optimal white-background reference:

| Concept | Reference Image |
|---------|-----------------|
| `chair` | ![Chair](milestones/m51_partcrafter_deploy/chair_ref.png) |
| `eagle` | ![Eagle](milestones/m51_partcrafter_deploy/eagle_ref.png) |
| `horse` | ![Horse](milestones/m51_partcrafter_deploy/horse_ref.png) |
| `butterfly` | ![Butterfly](milestones/m51_partcrafter_deploy/butterfly_ref.png) |
| `robot` | ![Robot](milestones/m51_partcrafter_deploy/robot_ref.png) |

</details>

<details>
<summary><strong>Architecture</strong></summary>

| Layer | Detail |
|-------|--------|
| **Models** | SDXL Turbo (~3 GB VRAM) + PartCrafter (~4 GB VRAM) = ~7 GB on L4 (24 GB) |
| **Preprocessing** | BriaRMBG background removal ‚Äî SDXL images have backgrounds, PartCrafter expects white-bg |
| **Mesh Output** | N part meshes per concept (e.g., 4 parts for chair: legs, seat, back, arms) |
| **Point Sampling** | `sample_from_part_meshes()` ‚Äî uniform surface sampling, 2048 points with part IDs |
| **Response Format** | Base64-encoded `Float32Array` positions + `Uint8Array` part IDs + part names JSON |
| **Vendoring** | PartCrafter tarball pinned to commit SHA `269bd41` for reproducible Docker builds |
| **Endpoint** | `POST /debug/generate-mesh` ‚Äî returns timing breakdown + part counts + vertex counts |

</details>

<details>
<summary><strong>Pipeline Timing Analysis</strong></summary>

> ‚ö†Ô∏è **PartCrafter is currently loading on the latest deploy.** The model downloads ~4 GB of weights from Hugging Face on cold start. Once loaded, mesh generation timing data will be added here.
>
> **Expected latency per concept (based on model specs):**

| Stage | Expected Latency |
|-------|------------------|
| SDXL Turbo (512√ó512, 1-step) | ~0.5‚Äì1.0s |
| BriaRMBG (background removal) | ~0.2‚Äì0.5s |
| PartCrafter (image ‚Üí part meshes) | ~30‚Äì60s |
| Point sampling (2048 pts) | ~5‚Äì10ms |
| **Total** | **~31‚Äì62s** |

PartCrafter dominates latency. The cache layer (entry #53) mitigates this for repeated concepts. The pre-generation script warms the top 50 concepts at deploy time.

</details>

<details>
<summary><strong>Deploy Status</strong></summary>

- SDXL Turbo: **loaded and serving** ‚úÖ
- PartCrafter: **loading on deploy** (downloading weights from Hugging Face) ‚è≥
- `/health/ready` shows `["sdxl_turbo"]` ‚Äî PartCrafter loads in background thread after SDXL Turbo
- Cache connected: ‚úÖ
- 5 reference images generated via live API confirming SDXL Turbo stage works end-to-end ‚úÖ
- 152 tests pass (32 PartCrafter + 120 existing), 0 regressions ‚úÖ

</details>

<details>
<summary><strong>Key Design Decisions</strong></summary>

- **Vendored PartCrafter** ‚Äî Not pip-installable; downloaded as a tarball pinned to commit SHA for reproducible Docker builds. Added to `PYTHONPATH` for `from src.*` imports.
- **BriaRMBG as separate timed step** ‚Äî SDXL Turbo images have backgrounds; PartCrafter expects white-bg. Background removal timing is logged independently.
- **Real mesh counting** ‚Äî None outputs from PartCrafter decoding failures are replaced with 1-vertex dummy meshes. Part count validation counts only real meshes (>1 vertex).
- **Sequential model loading** ‚Äî SDXL Turbo loads first (lighter, faster), then PartCrafter. Server serves SDXL-only requests while PartCrafter loads in background.
- **torch-cluster dependency** ‚Äî Required by PartCrafter for point cloud operations. Added to `pyproject.toml` with build-time torch access via `[tool.uv.extra-build-dependencies]`.

</details>

</details>

---

<details>
<summary><strong>53. S08: Cache Layer Hardening + Pre-Generation</strong></summary>

**Date:** 2026-02-22
**Branch:** `server/s08-cache`
**Commit:** `a7ee743`

<details>
<summary><strong>What Was Done</strong></summary>

Hardened the server-side `ShapeCache` and added infrastructure for pre-generation of common shapes:

1. **Thread-Safe LRU** ‚Äî Added `threading.Lock` around `cachetools.LRUCache` reads/writes for safe concurrent access when Cloud Run concurrency > 1.

2. **Thundering Herd Prevention** ‚Äî Per-key coalescing via `_in_flight: dict[str, asyncio.Event]`. Concurrent requests for the same uncached key await the result instead of issuing duplicate reads/generation.

3. **Cache Warming** ‚Äî `load_all_cached()` loads all pre-generated shapes from Cloud Storage into memory LRU on startup. 50 √ó ~27KB ‚âà 1.3MB, LRU evicts naturally.

4. **Pre-Generation Script** ‚Äî `scripts/pregenerate_top_50.py` is a post-deploy step that polls `/health/ready` before generating 50 common shapes. Idempotent ‚Äî skips cached concepts.

5. **Stats Endpoint** ‚Äî `GET /cache/stats` returns memory/storage sizes, hit counts, hit rate, and average retrieval times per tier.

6. **Collision Logging** ‚Äî Detects when two different texts normalize to the same hash key.

</details>

<details>
<summary><strong>Files Changed</strong></summary>

| File | Changes |
|------|---------|
| `server/app/cache/shape_cache.py` | **MODIFIED** ‚Äî Threading lock, coalescing, timing stats, `load_all_cached()`, `count_stored_shapes()`, collision tracking |
| `server/app/routes/cache.py` | **NEW** ‚Äî `GET /cache/stats` endpoint |
| `server/app/main.py` | **MODIFIED** ‚Äî Cache warming after model load, registered cache route |
| `server/scripts/pregenerate_top_50.py` | **NEW** ‚Äî Post-deploy script with health poll, progress logging |
| `server/tests/test_shape_cache.py` | **NEW** ‚Äî 24 tests |

</details>

<details>
<summary><strong>Verification</strong></summary>

- **176 server tests pass** (24 new + 152 existing), 0 regressions ‚úÖ

</details>

</details>

<details>
<summary><strong>54. A2: Parametric Primitive Library + Shader Enhancements</strong></summary>

> **Date:** 2026-02-22 ¬∑ **Branch:** `client/a2-primitives-shader`

<details>
<summary><strong>What Was Done</strong></summary>

Implemented the complete A2 task from the Client Animation Pipeline spec. This adds a parametric motion system to the existing particle physics ‚Äî 15 GLSL motion primitives that compute displacement vectors, fed into the existing spring-damper system for natural overshoot and settle.

**Key design decisions:**
- **Displacement into existing spring** ‚Äî primitives modify the spring target (`targetPos = restPos + displacement`), not velocity directly. The existing spring-damper provides free overshoot/settle/follow-through.
- **Data textures, not uniform arrays** ‚Äî motion plan params are stored in a 4√ó33 RGBA Float32 DataTexture. Avoids the vec4 slot explosion (`uniform float arr[N]` = 1 vec4 slot/element) and GLSL ES dynamically-uniform indexing constraint.
- **attachmentWeight** ‚Äî `tPartAttr` texture: R=partId (integer-valued float), G=attachmentWeight (0.0=joint, 1.0=extremity). Scales displacement for organic bends, not rigid block motion.
- **One-shot vs looping lifecycle** ‚Äî the *shader* handles duration. One-shots (`arc_translate`, `spring_settle`, `radial_burst`, `radial_contract`, `brownian_scatter`) compute `t = clamp((uTime - startTime) / duration, 0, 1)` and hold final displacement at `t=1`. Loopers (`duration=0`) use raw time. The template system just sets `startTime` + `duration` ‚Äî no polling needed.
- **Crossfade double-buffer** ‚Äî `tMotionPlanB` + `uBlendFactor` for smooth transitions. At `blendFactor=1.0`: copies plan B texture data ‚Üí plan A, resets blend to 0. No visual pop because output is already 100% plan B at that moment.
- **`#ifdef MOTION_PLAN_ENABLED`** ‚Äî `buildMotionPlanShader()` prepends `primitives.glsl` + `motion-plan.glsl` and adds `#define MOTION_PLAN_ENABLED`. If the prepend is removed, the velocity shader compiles and runs identically to pre-A2.
- **Cascaded dispatch** ‚Äî instead of a 15-way `switch` (which GLSL compilers flatten on mobile/Intel GPUs), uses nested 4-way `if/else` blocks. With ‚â§8 active primitives per frame, most particles share the same branch ‚Üí minimal divergence.

**Texture packing layout** ‚Äî `tMotionPlan` is 4 texels √ó 33 rows. Each row = 16 floats:

| Texel | Channels |
|-------|----------|
| 0 | `primitiveId, phase, startTime, duration` |
| 1 | `p0, p1, p2, p3` |
| 2 | `p4, p5, p6, p7` |
| 3 | `p8, p9, p10, p11` |

Row 0 = whole-body; rows 1‚Äì32 = per-part. Inactive parts: `primitiveId = -1`. Part ID 0 = unassigned (whole-body only).

</details>

<details>
<summary><strong>Files Changed</strong></summary>

| File | Changes |
|------|---------|
| `src/shaders/primitives.glsl` | **NEW** ‚Äî 15 GLSL motion primitives + 3 modifiers + cascaded 4-way dispatch |
| `src/shaders/motion-plan.glsl` | **NEW** ‚Äî Data texture read/dispatch, whole-body + per-part evaluation, crossfade |
| `src/shaders/velocity.frag.glsl` | **MODIFIED** ‚Äî `#ifdef MOTION_PLAN_ENABLED` guarded motion plan path + pitch stiffness |
| `src/renderer/types.ts` | **NEW** ‚Äî `MotionPlanData`, `PartMotionData` interfaces, primitive ID constants (0‚Äì14) |
| `src/engine/particle-system-extensions.ts` | **NEW** ‚Äî `MotionPlanManager`: texture packing, crossfade, `buildMotionPlanShader()` |
| `src/engine/ParticleSystem.ts` | **MODIFIED** ‚Äî Enhanced velocity shader, added `getVelocityUniforms()` |
| `src/test/primitive-test.html` | **NEW** ‚Äî Standalone test harness (GPUComputationRenderer, 6-part mock quadruped, sliders) |
| `src/__tests__/MotionPlanManager.test.ts` | **NEW** ‚Äî 20 tests |

</details>

<details>
<summary><strong>Verification</strong></summary>

- **89 tests pass** (20 new + 69 existing from A1), 0 regressions ‚úÖ

**Test coverage:** plan activation + uniform sync (3), texture packing layout verification (2), crossfade at t=0/0.5/1.0 including B‚ÜíA swap (3), `clearMotionPlan()` restore (1), part attribute texture R/G layout (1), types/constants/one-shot classification (4), `buildMotionPlanShader()` concatenation (3), construction defaults (2).

</details>

</details>
